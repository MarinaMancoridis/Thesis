{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4791e514",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook explores how false publications in science can emerge as the product of a multi-generational simulation of science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc4b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import beta, binom, entropy\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import scientist\n",
    "import evaluations\n",
    "\n",
    "# global variables\n",
    "num_bins = 30\n",
    "num_draws = 10\n",
    "num_participants = 100\n",
    "num_generations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63758e61",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6a78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print the scientific record\n",
    "def print_record(scientific_record):\n",
    "    print(\"Scientific record\")\n",
    "    for i in range(0, num_bins):\n",
    "        print(f\"   bin {i}: {scientific_record[i][0]} zero(s), {scientific_record[i][1]} one(s)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ac196",
   "metadata": {},
   "source": [
    "##  Reporting Settings\n",
    "A participant is in one of three settings for how they are allowed to report their data\n",
    "1. **Rate**: Report bin success rate\n",
    "2. **Data**: Report full results of bin samples\n",
    "3. **Subset**: Report partial results of bin samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb82fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportingSetting:\n",
    "    def __init__(self, name):\n",
    "        if name not in {\"rate\", \"data\", \"subset\"}:\n",
    "            raise ValueError(\"Improper setting name\")\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69256b7",
   "metadata": {},
   "source": [
    "## Sample bins\n",
    "To gather data, scientists choose actions (draws from a given bin) that maximize their expected value of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ff25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(draw_number, bin_sample_order, values_sampled, scientific_record):\n",
    "    # expected value of information for drawing from each of the bins\n",
    "    evis = {}\n",
    "    \n",
    "    for bin_num in range(0, num_bins):\n",
    "        count_zero_p = scientific_record[bin_num][0]\n",
    "        count_one_p = scientific_record[bin_num][1]\n",
    "        \n",
    "        count_zero_q = count_zero_p\n",
    "        count_one_q = count_one_p\n",
    "        \n",
    "        for i in range(0, len(bin_sample_order)):\n",
    "            if bin_sample_order[i] == bin_num:\n",
    "                value_drawn = values_sampled[i]\n",
    "                if value_drawn == 0:\n",
    "                    count_zero_q += 1\n",
    "                elif value_drawn == 1:\n",
    "                    count_one_q += 1\n",
    "    \n",
    "        payoff_zero = kl_divergence(count_zero_p, count_one_p, count_zero_q + 1, count_one_q)\n",
    "        payoff_one = kl_divergence(count_zero_p, count_one_p, count_zero_q, count_one_q + 1)\n",
    "        \n",
    "        prob_zero = count_zero_p / (count_zero_p + count_one_p)\n",
    "        prob_one = count_one_p / (count_zero_p + count_one_p)\n",
    "        evis[bin_num] = (prob_zero * payoff_zero) + (prob_one * payoff_one)\n",
    "    \n",
    "#         print(f\"      probability of sampling a zero: {prob_zero}\")\n",
    "#         print(f\"      probability of sampling a one: {prob_one}\")\n",
    "#         print(f\"      payoff of drawing a zero: {payoff_zero}\")\n",
    "#         print(f\"      payoff of drawing a one: {payoff_one}\")\n",
    "#         print(f\"      evi for sampling from bin {bin_num}: {evis[bin_num]}\\n\")\n",
    "    \n",
    "    # choose bin with the highest EVI to sample from\n",
    "    max_evi_bins = []\n",
    "    max_evi_value = None\n",
    "    for bin_num, evi_value in evis.items():\n",
    "        if max_evi_value is None or evi_value > max_evi_value:\n",
    "            max_evi_bins = [bin_num]  # start with a new list for a higher maximum\n",
    "            max_evi_value = evi_value\n",
    "        elif evi_value == max_evi_value:\n",
    "            max_evi_bins.append(bin_num)  # add bin to list in case of a tie\n",
    "\n",
    "    # If there are ties, choose a bin randomly from the tied bins\n",
    "    chosen_bin = random.choice(max_evi_bins)\n",
    "    \n",
    "    return chosen_bin, random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334a501",
   "metadata": {},
   "source": [
    "## Choose a bin\n",
    "\n",
    "Participants select the bin to report whose results maximize the KL divergence between the prior and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bff75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_bin(bin_sample_order, values_sampled, scientific_record):\n",
    "#     print(bin_sample_order)\n",
    "#     print(values_sampled)\n",
    "    \n",
    "    kl = {}\n",
    "    \n",
    "    for bin_num in range(0, num_bins):\n",
    "        count_zero_p = scientific_record[bin_num][0]\n",
    "        count_one_p = scientific_record[bin_num][1]\n",
    "        count_zero_q = count_zero_p\n",
    "        count_one_q = count_one_p\n",
    "        \n",
    "        for i in range(0, num_draws):\n",
    "            if bin_sample_order[i] == bin_num:\n",
    "                value_drawn = values_sampled[i]\n",
    "                if value_drawn == 0:\n",
    "                    count_zero_q += 1\n",
    "                elif value_drawn == 1:\n",
    "                    count_one_q += 1\n",
    "                    \n",
    "        kl[bin_num] = kl_divergence(count_zero_p, count_one_p, count_zero_q, count_one_q)\n",
    "        \n",
    "#         print(f\"   kl divergence of bin {bin_num}: {kl[bin_num]}\")\n",
    "\n",
    "    # choose bin with the highest kl divergence value\n",
    "    max_kl_bins = []\n",
    "    max_kl_value = None\n",
    "    for bin_num, kl_value in kl.items():\n",
    "        if max_kl_value is None or kl_value > max_kl_value:\n",
    "            max_kl_bins = [bin_num]  # start with a new list for a higher maximum\n",
    "            max_kl_value = kl_value\n",
    "        elif kl_value == max_kl_value:\n",
    "            max_kl_bins.append(bin_num)  # add bin to list in case of a tie\n",
    "\n",
    "    return random.choice(max_kl_bins) # choose random if there are ties for best bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a173c",
   "metadata": {},
   "source": [
    "## Report findings\n",
    "\n",
    "Participants report their findings, exaggerating their results by some degree $\\alpha$. When $\\alpha = 0$, this reduces to the strategy of reporting honest, unmanipulated results. When $\\alpha = 1$, this reduces to the strategy of reporting maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b187517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportingStrategy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def report(self, reporting_setting, alpha, bin_history):\n",
    "        num_zeros = bin_history.count(0)\n",
    "        num_ones = bin_history.count(1)\n",
    "        \n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "        \n",
    "        # overreport by a proportion of alpha of the remaining rate to get to a value of 1\n",
    "        if reporting_setting == \"rate\":\n",
    "            if num_ones + num_zeros == 0:\n",
    "                accurate_rate = 0.5\n",
    "            else:\n",
    "                accurate_rate = num_ones / (num_ones + num_zeros)\n",
    "            return(accurate_rate + alpha * (1 - accurate_rate))\n",
    "            \n",
    "        # overreport the number of '1's and underreport the number of '0's by a rate of alpha \n",
    "        elif reporting_setting == \"data\":\n",
    "            num_reported_zeros = round(num_zeros * (1 - alpha))\n",
    "            num_reported_ones = round(num_ones * (1 + alpha))\n",
    "            return({\"0\": num_reported_zeros, \"1\": num_reported_ones})\n",
    "        \n",
    "        # remove (100 * alpha)% of the '0' results\n",
    "        elif reporting_setting == \"subset\":\n",
    "            num_reported_zeros = round(num_zeros * (1 - alpha))\n",
    "            return({\"0\": num_reported_zeros, \"1\": num_ones})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f87143",
   "metadata": {},
   "source": [
    "## Participants\n",
    "Each participant has a personal records of draws and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b5d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Participant:\n",
    "    next_id = 1  # Class variable to keep track of the next available ID\n",
    "\n",
    "    def __init__(self, alpha, reporting_setting):        \n",
    "        self.id = Participant.next_id  # Assign a unique ID to the participant\n",
    "        Participant.next_id += 1  # Update the next available ID for the next participant\n",
    "\n",
    "        self.alpha = alpha                                                   # degree of exaggeration\n",
    "        self.reporting_setting = reporting_setting                           # type of report they can make\n",
    "        \n",
    "        self.strategy_report = ReportingStrategy()                           # how to report given samples\n",
    "        self.bin_sample_order = []                                           # order of bins sampled\n",
    "        self.values_sampled = []                                             # values received across draws\n",
    "        self.bin_choice = -1                                                 # the bin chosen to be reported\n",
    "        reported_results = None                                              # the results reported\n",
    "        \n",
    "    def sample(self, scientific_record):\n",
    "        sample_number = len(self.bin_sample_order)\n",
    "        bin_number, value = draw(len(self.values_sampled), self.bin_sample_order, self.values_sampled, scientific_record)\n",
    "        self.bin_sample_order.append(bin_number)\n",
    "        self.values_sampled.append(value)\n",
    "\n",
    "        return bin_number, value\n",
    "        \n",
    "    def choose_bin(self, scientific_record):\n",
    "        bin_choice = choose_bin(self.bin_sample_order, self.values_sampled, scientific_record)\n",
    "        self.bin_choice = bin_choice\n",
    "        \n",
    "        return bin_choice\n",
    "\n",
    "    def report(self, alpha):\n",
    "        history = get_full_history(self.bin_sample_order, self.values_sampled)\n",
    "        bin_history = history[num_draws - 1][self.bin_choice]\n",
    "        self.reported_results = self.strategy_report.report(self.reporting_setting.name, self.alpha, bin_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50c981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a data structure that shows, on each draw, the values seen in each bin at that point\n",
    "def get_full_history(bin_sample_order, values_sampled):\n",
    "    history = {draw_number: {bin_number: [] for bin_number in range(num_bins)} for draw_number in range(num_draws)}\n",
    "\n",
    "    for draw in range(len(bin_sample_order)):\n",
    "        if draw == 0:\n",
    "            history[draw][bin_sample_order[draw]].append(values_sampled[draw])\n",
    "        else:\n",
    "            prev_history = history[draw - 1].copy()\n",
    "            for bin_num in prev_history:\n",
    "                if bin_num == bin_sample_order[draw]:\n",
    "                    history[draw][bin_num] = prev_history[bin_num] + [values_sampled[draw]]\n",
    "                else:\n",
    "                    history[draw][bin_num] = prev_history[bin_num][:]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c3dc4",
   "metadata": {},
   "source": [
    "## Initialize participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dabb7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_participants(setting, alpha_value):\n",
    "    participants = []\n",
    "\n",
    "    for i in range (0, num_participants):\n",
    "        if setting == \"rate\":\n",
    "            report_set = ReportingSetting(\"rate\")\n",
    "        elif setting == \"data\":\n",
    "            report_set = ReportingSetting(\"data\")\n",
    "        elif setting == \"subset\":\n",
    "            report_set = ReportingSetting(\"subset\")\n",
    "\n",
    "        # make participant\n",
    "        participant = Participant(alpha=alpha_value, reporting_setting=report_set)\n",
    "                        \n",
    "        participants.append(participant)\n",
    "\n",
    "    return(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980de3fa",
   "metadata": {},
   "source": [
    "## Defining the peer review layer\n",
    "This layer takes in reports from scientists, selects reports for publication, and thereby updates the scientific record. The scientific record consists of the number of positive and negative draws associated with each bin. We do this because we assume that published data is given fully (not just publishing some sort of aggregation of the submitted results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7fb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the utility that a publisher assigns to a report, when the report is \n",
    "# accompanied by data (subset or full reporting styles)\n",
    "def utility_publish_data(report, scientific_record, bin_choice):    \n",
    "    # bump by amount of supporting data\n",
    "    score = report[\"0\"] + report[\"1\"]\n",
    "    \n",
    "    # bump by level of surprise\n",
    "    count_zero_p = scientific_record[bin_choice][0]\n",
    "    count_one_p = scientific_record[bin_choice][1]\n",
    "    kl = kl_divergence(count_zero_p, count_one_p, count_zero_p + report[\"0\"], count_one_p + report[\"1\"])\n",
    "    score += 10 * kl\n",
    "\n",
    "    # bump by publication bias (+5% if positive and -5% if negative)\n",
    "    if report[\"0\"] > report[\"1\"]:\n",
    "        score = 0.95 * score\n",
    "    else:\n",
    "        score = 1.05 * score\n",
    "\n",
    "#     print(f\"final score {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46006178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the utility that a publisher assigns to a report, when the report\n",
    "# consists soley of a rate\n",
    "def utility_publish_rate(report, scientific_record, bin_choice):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4959c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update the published scientific record\n",
    "# the peer review board selects reports for publication and returns the updated scientific record\n",
    "def peer_review(participants, scientific_record):  \n",
    "    actor_optimality = 1\n",
    "    id_to_prob = {}\n",
    "    total = 0\n",
    "    \n",
    "    for participant in participants:\n",
    "        report_util = utility_publish_data(participant.reported_results, scientific_record, participant.bin_choice)\n",
    "        report_prob = math.exp(actor_optimality * report_util)\n",
    "        id_to_prob[participant.id] = report_prob\n",
    "        total += report_prob\n",
    "    \n",
    "    for prob in id_to_prob:\n",
    "        id_to_prob[prob] /= total\n",
    "    \n",
    "    # publish 20% of the submitted reports\n",
    "    number_published = math.ceil(len(participants)/5)\n",
    "    participant_ids = list(id_to_prob.keys())\n",
    "    probabilities = list(id_to_prob.values())\n",
    "\n",
    "    for i in range(0, number_published):\n",
    "        selected_participant = random.choices(participant_ids, probabilities)[0]\n",
    "#         print(f\"participant selected: {selected_participant}\")\n",
    "        \n",
    "        # update scientific record\n",
    "        for p in participants:\n",
    "            if p.id == selected_participant:\n",
    "                scientific_record[p.bin_choice][0] += p.reported_results[\"0\"]\n",
    "                scientific_record[p.bin_choice][1] += p.reported_results[\"1\"]\n",
    "                \n",
    "        # remove that participant from the list\n",
    "        selected_index = participant_ids.index(selected_participant)\n",
    "        del participant_ids[selected_index]\n",
    "        del probabilities[selected_index]\n",
    "        \n",
    "        # Re-normalize the probabilities\n",
    "        total_prob = sum(probabilities)\n",
    "        probabilities = [prob / total_prob for prob in probabilities]\n",
    "\n",
    "    return scientific_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911151",
   "metadata": {},
   "source": [
    "## Run an experiment\n",
    "\n",
    "The multi-generational experiment is run, given reporting setting and exaggeration values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e60e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(setting, alpha_value):\n",
    "    # each experiment starts with a blank cannon (starts with 1-1 prior)\n",
    "    scientific_record = {}\n",
    "    for bin_num in range(0, num_bins):\n",
    "        scientific_record[bin_num] = {} \n",
    "        scientific_record[bin_num][0] = 1\n",
    "        scientific_record[bin_num][1] = 1\n",
    "    \n",
    "    for generation in range(0, num_generations):\n",
    "        print(f\"* Generation {generation}...\")\n",
    "        print_record(scientific_record)\n",
    "        \n",
    "        # each generation gets an entirely new set of participants\n",
    "        participants = make_participants(setting, alpha_value)\n",
    "\n",
    "        # scientists explore and submit reports\n",
    "        for participant in participants:\n",
    "            # sample\n",
    "            for i in range(0, num_draws):\n",
    "                bin_number, value = participant.sample(scientific_record)\n",
    "                \n",
    "#                 print(f\"   sample from bin {bin_number}: {value}\")\n",
    "\n",
    "            # choose the bin\n",
    "            bin_choice = participant.choose_bin(scientific_record)\n",
    "            print(f\"   chose bin {bin_choice}\")\n",
    "\n",
    "            # specify alpha value\n",
    "            participant.report(alpha_value)\n",
    "            \n",
    "        # the peer review board selects reports for publication and returns the updated scientific record\n",
    "        scientific_record = peer_review(participants, scientific_record)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ce5884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Generation 0...\n",
      "Scientific record\n",
      "   bin 0: 1 zero(s), 1 one(s)\n",
      "   bin 1: 1 zero(s), 1 one(s)\n",
      "   bin 2: 1 zero(s), 1 one(s)\n",
      "   bin 3: 1 zero(s), 1 one(s)\n",
      "   bin 4: 1 zero(s), 1 one(s)\n",
      "   bin 5: 1 zero(s), 1 one(s)\n",
      "   bin 6: 1 zero(s), 1 one(s)\n",
      "   bin 7: 1 zero(s), 1 one(s)\n",
      "   bin 8: 1 zero(s), 1 one(s)\n",
      "   bin 9: 1 zero(s), 1 one(s)\n",
      "   bin 10: 1 zero(s), 1 one(s)\n",
      "   bin 11: 1 zero(s), 1 one(s)\n",
      "   bin 12: 1 zero(s), 1 one(s)\n",
      "   bin 13: 1 zero(s), 1 one(s)\n",
      "   bin 14: 1 zero(s), 1 one(s)\n",
      "   bin 15: 1 zero(s), 1 one(s)\n",
      "   bin 16: 1 zero(s), 1 one(s)\n",
      "   bin 17: 1 zero(s), 1 one(s)\n",
      "   bin 18: 1 zero(s), 1 one(s)\n",
      "   bin 19: 1 zero(s), 1 one(s)\n",
      "   bin 20: 1 zero(s), 1 one(s)\n",
      "   bin 21: 1 zero(s), 1 one(s)\n",
      "   bin 22: 1 zero(s), 1 one(s)\n",
      "   bin 23: 1 zero(s), 1 one(s)\n",
      "   bin 24: 1 zero(s), 1 one(s)\n",
      "   bin 25: 1 zero(s), 1 one(s)\n",
      "   bin 26: 1 zero(s), 1 one(s)\n",
      "   bin 27: 1 zero(s), 1 one(s)\n",
      "   bin 28: 1 zero(s), 1 one(s)\n",
      "   bin 29: 1 zero(s), 1 one(s)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kl_divergence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 20\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(setting, alpha_value)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m participant \u001b[38;5;129;01min\u001b[39;00m participants:\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_draws):\n\u001b[0;32m---> 20\u001b[0m                 bin_number, value \u001b[38;5;241m=\u001b[39m \u001b[43mparticipant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscientific_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#                 print(f\"   sample from bin {bin_number}: {value}\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m             \u001b[38;5;66;03m# choose the bin\u001b[39;00m\n\u001b[1;32m     25\u001b[0m             bin_choice \u001b[38;5;241m=\u001b[39m participant\u001b[38;5;241m.\u001b[39mchoose_bin(scientific_record)\n",
      "Cell \u001b[0;32mIn [7], line 19\u001b[0m, in \u001b[0;36mParticipant.sample\u001b[0;34m(self, scientific_record)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, scientific_record):\n\u001b[1;32m     18\u001b[0m     sample_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbin_sample_order)\n\u001b[0;32m---> 19\u001b[0m     bin_number, value \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues_sampled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbin_sample_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues_sampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscientific_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbin_sample_order\u001b[38;5;241m.\u001b[39mappend(bin_number)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues_sampled\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(draw_number, bin_sample_order, values_sampled, scientific_record)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m value_drawn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     18\u001b[0m             count_one_q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m payoff_zero \u001b[38;5;241m=\u001b[39m \u001b[43mkl_divergence\u001b[49m(count_zero_p, count_one_p, count_zero_q \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, count_one_q)\n\u001b[1;32m     21\u001b[0m payoff_one \u001b[38;5;241m=\u001b[39m kl_divergence(count_zero_p, count_one_p, count_zero_q, count_one_q \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m prob_zero \u001b[38;5;241m=\u001b[39m count_zero_p \u001b[38;5;241m/\u001b[39m (count_zero_p \u001b[38;5;241m+\u001b[39m count_one_p)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kl_divergence' is not defined"
     ]
    }
   ],
   "source": [
    "run_experiment(\"data\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56369a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8543b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84227cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49345d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439ad45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd3ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e382ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aefdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc73f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65afba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f905bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf7902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d22ab4a9",
   "metadata": {},
   "source": [
    "## Compare hyperparameters across experiments\n",
    "\n",
    "Run multiple experiments to see the effect of different reporting settings and alpha values on the published scientific record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_settings = [\"rate\", \"data\", \"subset\"]\n",
    "alpha_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "results = [] # TODO: fix this into whatever you want\n",
    "\n",
    "for reporting_setting in reporting_settings:\n",
    "    for alpha_value in alpha_values:\n",
    "        result = run_experiment(reporting_strategy, setting, alpha_value)\n",
    "        results.append(result)\n",
    "\n",
    "        # TODO: save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22a113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eee9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b3d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c0d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864a267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b89ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a176cd4e",
   "metadata": {},
   "source": [
    "## Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary to store mean percent errors\n",
    "mean_percent_errors_dict = {}\n",
    "\n",
    "# Number of runs for each key\n",
    "num_runs = 10\n",
    "\n",
    "for gathering_strategy in gathering_strategies:\n",
    "    for bin_choosing_strategy in bin_choosing_strategies:\n",
    "        for reporting_strategy in reporting_strategies:\n",
    "            for setting in reporting_setting:\n",
    "                for alpha_value in alpha_values:\n",
    "                    # Initialize a list to store MPE for each run\n",
    "                    mpe_list = []\n",
    "\n",
    "                    for _ in range(num_runs):\n",
    "                        participants = make_participants(gathering_strategy, bin_choosing_strategy, reporting_strategy, setting, alpha_value)\n",
    "                        mean_percent_error = peer_review(participants)\n",
    "                        mpe_list.append(mean_percent_error)\n",
    "\n",
    "                    # Calculate the average MPE\n",
    "                    avg_mpe = np.mean(mpe_list)\n",
    "\n",
    "                    # Create a key based on the variable names\n",
    "                    key = (gathering_strategy, bin_choosing_strategy, reporting_strategy, setting, alpha_value)\n",
    "\n",
    "                    # Store the average MPE in the dictionary\n",
    "                    mean_percent_errors_dict[key] = avg_mpe\n",
    "\n",
    "print(mean_percent_errors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Specify the file path to save the JSON file\n",
    "json_file_path = 'mean_percent_errors.json'\n",
    "\n",
    "# Save the dictionary with string keys to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(string_keys_dict, json_file)\n",
    "\n",
    "print(f\"Mean percent errors saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52387797",
   "metadata": {},
   "source": [
    "## Find best 5 and worst 5 settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edca4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Sort the dictionary by values\n",
    "sorted_dict = dict(sorted(string_keys_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Print the best five settings with their MPE\n",
    "print(\"Best 15 Settings:\")\n",
    "for key in list(sorted_dict)[:15]:\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    mpe = sorted_dict[key]\n",
    "    print(f\"{setting_tuple}: {mpe}\")\n",
    "\n",
    "# Print the worst five settings with their MPE\n",
    "print(\"\\nWorst 15 Settings:\")\n",
    "for key in list(sorted_dict)[-15:]:\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    mpe = sorted_dict[key]\n",
    "    print(f\"{setting_tuple}: {mpe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb73cdcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_percent_errors_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert tuple keys to strings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m string_keys_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmean_percent_errors_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Filter out settings where alpha is equal to 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m filtered_dict \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m string_keys_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_percent_errors_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Filter out settings where alpha is equal to 1\n",
    "filtered_dict = {key: value for key, value in string_keys_dict.items() if eval(key)[-1] != 1}\n",
    "\n",
    "# Sort the filtered dictionary by values in descending order\n",
    "sorted_filtered_dict = dict(sorted(filtered_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "r\n",
    "# Print the worst settings with alpha not equal to 1\n",
    "print(\"Worst Settings (alpha not equal to 1) in Order:\")\n",
    "for key, value in sorted_filtered_dict.items():\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    print(f\"{setting_tuple}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Create a defaultdict to store MPE values for each unique setting component\n",
    "component_mpes = defaultdict(list)\n",
    "\n",
    "# Populate the defaultdict with MPE values\n",
    "for key, value in string_keys_dict.items():\n",
    "    setting_tuple = eval(key)\n",
    "    \n",
    "    # Iterate over all components in the setting tuple\n",
    "    for component in setting_tuple:\n",
    "        component_mpes[component].append(value)\n",
    "\n",
    "# Calculate average MPE for each unique setting component\n",
    "average_mpes = {component: np.mean(mpe_list) for component, mpe_list in component_mpes.items()}\n",
    "\n",
    "# Print the average MPE for each setting component\n",
    "print(\"Average MPE for Each Setting Component:\")\n",
    "for component, average_mpe in average_mpes.items():\n",
    "    print(f\"{component}: {average_mpe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
