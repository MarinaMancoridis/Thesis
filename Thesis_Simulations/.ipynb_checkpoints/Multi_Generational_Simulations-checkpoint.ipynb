{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4791e514",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook explores how false publications in science can emerge as the product of a multi-generational simulation of science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc4b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import beta, binom, entropy\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# my modules\n",
    "import scientist\n",
    "import evaluation\n",
    "import helper\n",
    "import settings\n",
    "import publisher\n",
    "\n",
    "# global variables\n",
    "num_bins = 4\n",
    "num_draws = 10\n",
    "num_participants = 10\n",
    "num_generations = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c3dc4",
   "metadata": {},
   "source": [
    "## Initialize participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabb7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_participants(setting, alpha_value):\n",
    "    participants = []\n",
    "\n",
    "    for i in range (0, num_participants):\n",
    "        if setting == \"rate\":\n",
    "            report_set = settings.ReportingSetting(\"rate\")\n",
    "        elif setting == \"data\":\n",
    "            report_set = settings.ReportingSetting(\"data\")\n",
    "        elif setting == \"subset\":\n",
    "            report_set = settings.ReportingSetting(\"subset\")\n",
    "\n",
    "        # make participant\n",
    "        participant = scientist.Participant(alpha=alpha_value, reporting_setting=report_set)\n",
    "                        \n",
    "        participants.append(participant)\n",
    "\n",
    "    return(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911151",
   "metadata": {},
   "source": [
    "## Run an experiment\n",
    "\n",
    "The multi-generational experiment is run, given reporting setting and exaggeration values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e60e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(setting, alpha_value):\n",
    "    # each experiment starts with a blank cannon (starts with 1-1 prior)\n",
    "    scientific_record = {}\n",
    "    for bin_num in range(0, num_bins):\n",
    "        scientific_record[bin_num] = {} \n",
    "        scientific_record[bin_num][0] = 1\n",
    "        scientific_record[bin_num][1] = 1\n",
    "    \n",
    "    for generation in range(0, num_generations):\n",
    "        print(f\"* Generation {generation}...\")\n",
    "        helper.print_record(scientific_record, num_bins)\n",
    "        \n",
    "        # each generation gets an entirely new set of participants\n",
    "        participants = make_participants(setting, alpha_value)\n",
    "\n",
    "        # scientists explore and submit reports\n",
    "        for participant in participants:\n",
    "            # sample\n",
    "            for i in range(0, num_draws):\n",
    "                bin_number, value = participant.sample(scientific_record, num_bins)\n",
    "                \n",
    "#                 print(f\"   sample from bin {bin_number}: {value}\")\n",
    "\n",
    "            # choose the bin\n",
    "            bin_choice = participant.choose_bin(scientific_record, num_bins, num_draws)\n",
    "            print(f\"   chose bin {bin_choice}\")\n",
    "\n",
    "            # make a report\n",
    "            participant.report(num_bins, num_draws)\n",
    "            \n",
    "        # the peer review board selects reports for publication and returns the updated scientific record\n",
    "        scientific_record = publisher.peer_review(participants, scientific_record)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ce5884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Generation 0...\n",
      "Scientific record\n",
      "   bin 0: 1 zero(s), 1 one(s)\n",
      "   bin 1: 1 zero(s), 1 one(s)\n",
      "   bin 2: 1 zero(s), 1 one(s)\n",
      "   bin 3: 1 zero(s), 1 one(s)\n",
      "\n",
      "   chose bin 2\n",
      "   chose bin 1\n",
      "   chose bin 1\n",
      "   chose bin 1\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 0\n",
      "   chose bin 1\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "participant selected: 1\n",
      "participant selected: 8\n",
      "\n",
      "* Generation 1...\n",
      "Scientific record\n",
      "   bin 0: 1 zero(s), 1 one(s)\n",
      "   bin 1: 1 zero(s), 7 one(s)\n",
      "   bin 2: 3 zero(s), 9 one(s)\n",
      "   bin 3: 1 zero(s), 1 one(s)\n",
      "\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "   chose bin 3\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "   chose bin 0\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "participant selected: 15\n",
      "participant selected: 11\n",
      "\n",
      "* Generation 2...\n",
      "Scientific record\n",
      "   bin 0: 1 zero(s), 19 one(s)\n",
      "   bin 1: 1 zero(s), 7 one(s)\n",
      "   bin 2: 3 zero(s), 9 one(s)\n",
      "   bin 3: 1 zero(s), 1 one(s)\n",
      "\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 3\n",
      "   chose bin 0\n",
      "participant selected: 29\n",
      "participant selected: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"data\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56369a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8543b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84227cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49345d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd3ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e382ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aefdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc73f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65afba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f905bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf7902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d22ab4a9",
   "metadata": {},
   "source": [
    "## Compare hyperparameters across experiments\n",
    "\n",
    "Run multiple experiments to see the effect of different reporting settings and alpha values on the published scientific record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_settings = [\"rate\", \"data\", \"subset\"]\n",
    "alpha_values = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "results = [] # TODO: fix this into whatever you want\n",
    "\n",
    "for reporting_setting in reporting_settings:\n",
    "    for alpha_value in alpha_values:\n",
    "        result = run_experiment(reporting_strategy, setting, alpha_value)\n",
    "        results.append(result)\n",
    "\n",
    "        # TODO: save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22a113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eee9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b3d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c0d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864a267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b89ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a176cd4e",
   "metadata": {},
   "source": [
    "## Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary to store mean percent errors\n",
    "mean_percent_errors_dict = {}\n",
    "\n",
    "# Number of runs for each key\n",
    "num_runs = 10\n",
    "\n",
    "for gathering_strategy in gathering_strategies:\n",
    "    for bin_choosing_strategy in bin_choosing_strategies:\n",
    "        for reporting_strategy in reporting_strategies:\n",
    "            for setting in reporting_setting:\n",
    "                for alpha_value in alpha_values:\n",
    "                    # Initialize a list to store MPE for each run\n",
    "                    mpe_list = []\n",
    "\n",
    "                    for _ in range(num_runs):\n",
    "                        participants = make_participants(gathering_strategy, bin_choosing_strategy, reporting_strategy, setting, alpha_value)\n",
    "                        mean_percent_error = peer_review(participants)\n",
    "                        mpe_list.append(mean_percent_error)\n",
    "\n",
    "                    # Calculate the average MPE\n",
    "                    avg_mpe = np.mean(mpe_list)\n",
    "\n",
    "                    # Create a key based on the variable names\n",
    "                    key = (gathering_strategy, bin_choosing_strategy, reporting_strategy, setting, alpha_value)\n",
    "\n",
    "                    # Store the average MPE in the dictionary\n",
    "                    mean_percent_errors_dict[key] = avg_mpe\n",
    "\n",
    "print(mean_percent_errors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Specify the file path to save the JSON file\n",
    "json_file_path = 'mean_percent_errors.json'\n",
    "\n",
    "# Save the dictionary with string keys to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(string_keys_dict, json_file)\n",
    "\n",
    "print(f\"Mean percent errors saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52387797",
   "metadata": {},
   "source": [
    "## Find best 5 and worst 5 settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edca4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Sort the dictionary by values\n",
    "sorted_dict = dict(sorted(string_keys_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Print the best five settings with their MPE\n",
    "print(\"Best 15 Settings:\")\n",
    "for key in list(sorted_dict)[:15]:\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    mpe = sorted_dict[key]\n",
    "    print(f\"{setting_tuple}: {mpe}\")\n",
    "\n",
    "# Print the worst five settings with their MPE\n",
    "print(\"\\nWorst 15 Settings:\")\n",
    "for key in list(sorted_dict)[-15:]:\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    mpe = sorted_dict[key]\n",
    "    print(f\"{setting_tuple}: {mpe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb73cdcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_percent_errors_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert tuple keys to strings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m string_keys_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmean_percent_errors_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Filter out settings where alpha is equal to 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m filtered_dict \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m string_keys_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_percent_errors_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Filter out settings where alpha is equal to 1\n",
    "filtered_dict = {key: value for key, value in string_keys_dict.items() if eval(key)[-1] != 1}\n",
    "\n",
    "# Sort the filtered dictionary by values in descending order\n",
    "sorted_filtered_dict = dict(sorted(filtered_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "r\n",
    "# Print the worst settings with alpha not equal to 1\n",
    "print(\"Worst Settings (alpha not equal to 1) in Order:\")\n",
    "for key, value in sorted_filtered_dict.items():\n",
    "    setting_tuple = eval(key)  # Convert the string back to a tuple\n",
    "    print(f\"{setting_tuple}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Convert tuple keys to strings\n",
    "string_keys_dict = {str(key): value for key, value in mean_percent_errors_dict.items()}\n",
    "\n",
    "# Create a defaultdict to store MPE values for each unique setting component\n",
    "component_mpes = defaultdict(list)\n",
    "\n",
    "# Populate the defaultdict with MPE values\n",
    "for key, value in string_keys_dict.items():\n",
    "    setting_tuple = eval(key)\n",
    "    \n",
    "    # Iterate over all components in the setting tuple\n",
    "    for component in setting_tuple:\n",
    "        component_mpes[component].append(value)\n",
    "\n",
    "# Calculate average MPE for each unique setting component\n",
    "average_mpes = {component: np.mean(mpe_list) for component, mpe_list in component_mpes.items()}\n",
    "\n",
    "# Print the average MPE for each setting component\n",
    "print(\"Average MPE for Each Setting Component:\")\n",
    "for component, average_mpe in average_mpes.items():\n",
    "    print(f\"{component}: {average_mpe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
