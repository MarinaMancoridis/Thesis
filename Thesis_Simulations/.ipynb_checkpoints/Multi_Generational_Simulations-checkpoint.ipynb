{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4791e514",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook explores how false publications in science can emerge as the product of a multi-generational simulation of science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc4b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import beta, binom, entropy\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# my modules\n",
    "import scientist\n",
    "import evaluation\n",
    "import helper\n",
    "import settings\n",
    "import publisher\n",
    "\n",
    "# global variables\n",
    "num_bins = 3\n",
    "num_draws = 10\n",
    "num_participants = 10\n",
    "num_generations = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558aa700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of bins\n",
    "bins_to_probs = {}\n",
    "for i in range(0, num_bins):\n",
    "    bins_to_probs[i] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c3dc4",
   "metadata": {},
   "source": [
    "## Initialize participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabb7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_participants(setting, alpha_value):\n",
    "    participants = []\n",
    "\n",
    "    for i in range (0, num_participants):\n",
    "        if setting == \"rate\":\n",
    "            report_set = settings.ReportingSetting(\"rate\")\n",
    "        elif setting == \"data\":\n",
    "            report_set = settings.ReportingSetting(\"data\")\n",
    "        elif setting == \"subset\":\n",
    "            report_set = settings.ReportingSetting(\"subset\")\n",
    "\n",
    "        # make participant\n",
    "        participant = scientist.Participant(alpha=alpha_value, reporting_setting=report_set)\n",
    "                        \n",
    "        participants.append(participant)\n",
    "\n",
    "    return(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39911151",
   "metadata": {},
   "source": [
    "## Run an experiment\n",
    "\n",
    "The multi-generational experiment is run, given reporting setting and exaggeration values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e60e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(setting, alpha_value, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val):\n",
    "    # each experiment starts with a blank cannon (starts with 1-1 prior)\n",
    "    scientific_record = {}\n",
    "    for bin_num in range(0, num_bins):\n",
    "        scientific_record[bin_num] = {} \n",
    "        scientific_record[bin_num][0] = 1\n",
    "        scientific_record[bin_num][1] = 1\n",
    "    \n",
    "    for generation in range(0, num_generations):\n",
    "#         print(f\"\\n* Generation {generation}...\")\n",
    "#         helper.print_record(scientific_record, num_bins)\n",
    "#         print(f\"   Arm score: {evaluation.arm_parameter_score(scientific_record, bins_to_probs)}\")\n",
    "#         print(f\"   Entropy score: {evaluation.total_entropy_score(scientific_record, bins_to_probs)}\")\n",
    "        \n",
    "        # each generation gets an entirely new set of participants\n",
    "        participants = make_participants(setting, alpha_value)\n",
    "\n",
    "        # scientists explore and submit reports\n",
    "        for participant in participants:\n",
    "            # sample\n",
    "            for i in range(0, num_draws):\n",
    "                bin_number, value = participant.sample(scientific_record, num_bins, bins_to_probs)\n",
    "                \n",
    "#                 print(f\"   sample from bin {bin_number}: {value}\")\n",
    "\n",
    "            # choose the bin\n",
    "            bin_choice = participant.choose_bin(scientific_record, num_bins, num_draws)\n",
    "#             print(f\"   chose bin {bin_choice}\")\n",
    "\n",
    "            # make a report\n",
    "            participant.report(num_bins, num_draws)\n",
    "            \n",
    "        # the peer review board selects reports for publication and returns the updated scientific record\n",
    "        scientific_record = publisher.peer_review(participants, scientific_record, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val, num_draws)\n",
    "        \n",
    "#     print(\"\\n\\n* FINAL RESULTS\")\n",
    "#     helper.print_record(scientific_record, num_bins)\n",
    "    \n",
    "#     # final metric of how well scientists play the multi-armed bandit game\n",
    "#     print(evaluation.arm_parameter_score(scientific_record, bins_to_probs))\n",
    "\n",
    "#     # final metric of how well scientists reduce the entropy of the scientific record\n",
    "#     print(evaluation.total_entropy_score(scientific_record, bins_to_probs))\n",
    "    \n",
    "    return(evaluation.arm_parameter_score(scientific_record, bins_to_probs), evaluation.total_entropy_score(scientific_record, bins_to_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ce5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.012726142997514603, 0.00047517397754083535)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting, alpha_value, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val\n",
    "run_experiment(\"data\", 0, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ddb89",
   "metadata": {},
   "source": [
    "## Searching over the space of publishing policies\n",
    "\n",
    "Search across relative weights of how much data is associated with a report, how surprising the report is, and publication bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f8e1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale over amount of supporting data \n",
    "rel_pl_data = np.linspace(0, 10, 21)\n",
    "\n",
    "# scale over how surprising the data is\n",
    "rel_pl_surprise = np.linspace(0, 10, 21)\n",
    "\n",
    "# rate of bump for publication bias (0.01 = 1% publication bias)\n",
    "rel_pl_bias = np.linspace(0, 1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8521b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining value: 0.0\n",
      "examining value: 0.5\n",
      "examining value: 1.0\n",
      "examining value: 1.5\n",
      "examining value: 2.0\n",
      "examining value: 2.5\n",
      "examining value: 3.0\n",
      "examining value: 3.5\n",
      "examining value: 4.0\n",
      "examining value: 4.5\n",
      "examining value: 5.0\n",
      "examining value: 5.5\n",
      "examining value: 6.0\n",
      "examining value: 6.5\n",
      "examining value: 7.0\n",
      "examining value: 7.5\n",
      "examining value: 8.0\n",
      "examining value: 8.5\n",
      "examining value: 9.0\n",
      "examining value: 9.5\n",
      "examining value: 10.0\n"
     ]
    }
   ],
   "source": [
    "publishing_policies_space = {}\n",
    "exp_no = 0\n",
    "\n",
    "for rel_pl_data_val in rel_pl_data:\n",
    "    print(f\"examining value: {rel_pl_data_val}\")\n",
    "    for rel_pl_surprise_val in rel_pl_surprise:\n",
    "        for rel_pl_bias_val in rel_pl_bias:\n",
    "            total_arm_score = 0\n",
    "            total_entropy_score = 0\n",
    "           \n",
    "            for i in range(0, 5):\n",
    "                arm_score, entropy_score = run_experiment(\"data\", 0, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val)\n",
    "                total_arm_score += arm_score\n",
    "                total_entropy_score += entropy_score\n",
    "            \n",
    "            key = (rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val)\n",
    "            publishing_policies_space[key] = [total_arm_score / 5, total_entropy_score / 5] # average over 5 runs of each combination\n",
    "            exp_no += 1\n",
    "            \n",
    "# save the results\n",
    "pickle.dump(\n",
    "    publishing_policies_space,\n",
    "    open(\"/Users/marinamancoridis/Thesis/Thesis_Simulations/publishing_policies_3_bins.p\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423e6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "num_bins = 30\n",
    "num_draws = 10\n",
    "num_participants = 10\n",
    "num_generations = 5\n",
    "\n",
    "# distribution of bins\n",
    "bins_to_probs = {}\n",
    "for i in range(0, num_bins):\n",
    "    bins_to_probs[i] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ab38b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining value: 0.0\n",
      "examining value: 0.5\n",
      "examining value: 1.0\n",
      "examining value: 1.5\n",
      "examining value: 2.0\n",
      "examining value: 2.5\n",
      "examining value: 3.0\n",
      "examining value: 3.5\n",
      "examining value: 4.0\n",
      "examining value: 4.5\n",
      "examining value: 5.0\n",
      "examining value: 5.5\n",
      "examining value: 6.0\n",
      "examining value: 6.5\n",
      "examining value: 7.0\n",
      "examining value: 7.5\n",
      "examining value: 8.0\n",
      "examining value: 8.5\n",
      "examining value: 9.0\n",
      "examining value: 9.5\n",
      "examining value: 10.0\n"
     ]
    }
   ],
   "source": [
    "publishing_policies_space = {}\n",
    "exp_no = 0\n",
    "\n",
    "for rel_pl_data_val in rel_pl_data:\n",
    "    print(f\"examining value: {rel_pl_data_val}\")\n",
    "    for rel_pl_surprise_val in rel_pl_surprise:\n",
    "        for rel_pl_bias_val in rel_pl_bias:\n",
    "            total_arm_score = 0\n",
    "            total_entropy_score = 0\n",
    "\n",
    "            for i in range(0, 5):\n",
    "                arm_score, entropy_score = run_experiment(\"data\", 0, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val)\n",
    "                total_arm_score += arm_score\n",
    "                total_entropy_score += entropy_score\n",
    "\n",
    "            key = (rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val)\n",
    "            publishing_policies_space[key] = [total_arm_score / 5, total_entropy_score / 5] # average over 5 runs of each combination\n",
    "            exp_no += 1\n",
    "\n",
    "# save the results\n",
    "pickle.dump(\n",
    "    publishing_policies_space,\n",
    "    open(f\"/Users/marinamancoridis/Thesis/Thesis_Simulations/publishing_policies_30_bins.p\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45e94b",
   "metadata": {},
   "source": [
    "## Fix a setting and look at how the evaluation metric changes over generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e612be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "num_bins = 30\n",
    "num_draws = 10\n",
    "num_participants = 10\n",
    "num_generations = 50\n",
    "\n",
    "# distribution of bins\n",
    "bins_to_probs = {}\n",
    "for i in range(0, num_bins):\n",
    "    bins_to_probs[i] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3942513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_gen_info(setting, alpha_value, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val):\n",
    "    scientific_record = {}\n",
    "    for bin_num in range(0, num_bins):\n",
    "        scientific_record[bin_num] = {} \n",
    "        scientific_record[bin_num][0] = 1\n",
    "        scientific_record[bin_num][1] = 1\n",
    "    \n",
    "    kl_per_gen = {}\n",
    "    \n",
    "    for generation in range(0, num_generations):       \n",
    "        # each generation gets an entirely new set of participants\n",
    "        participants = make_participants(setting, alpha_value)\n",
    "\n",
    "        # scientists explore and submit reports\n",
    "        for participant in participants:\n",
    "            # sample\n",
    "            for i in range(0, num_draws):\n",
    "                bin_number, value = participant.sample(scientific_record, num_bins, bins_to_probs)\n",
    "\n",
    "            # choose the bin\n",
    "            bin_choice = participant.choose_bin(scientific_record, num_bins, num_draws)\n",
    "\n",
    "            # make a report\n",
    "            participant.report(num_bins, num_draws)\n",
    "            \n",
    "        # the peer review board selects reports for publication and returns the updated scientific record\n",
    "        scientific_record = publisher.peer_review(participants, scientific_record, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val, num_draws)\n",
    "        \n",
    "        kl_per_gen[generation] = evaluation.total_entropy_score(scientific_record, bins_to_probs)\n",
    "    \n",
    "    return(evaluation.arm_parameter_score(scientific_record, bins_to_probs), evaluation.total_entropy_score(scientific_record, bins_to_probs), kl_per_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae32ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "final_map = {}\n",
    "for i in range(0, len(values)):\n",
    "    final_map[values[i]] = {}\n",
    "    \n",
    "for value in values:\n",
    "    print(value)\n",
    "    # distribution of bins\n",
    "    bins_to_probs = {}\n",
    "    for i in range(0, num_bins):\n",
    "        bins_to_probs[i] = value\n",
    "\n",
    "    average_KL_per_generation = {}\n",
    "    for i in range(0, num_generations):\n",
    "        average_KL_per_generation[i] = 0\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        arm_score, entropy_score, kl_per_gen = run_experiment_gen_info(\"data\", 0, 1, 1, 0)\n",
    "        for gen_no in kl_per_gen:\n",
    "            average_KL_per_generation[gen_no] += kl_per_gen[gen_no]\n",
    "\n",
    "    for key in average_KL_per_generation:\n",
    "        average_KL_per_generation[key] /= 5\n",
    "        \n",
    "    final_map[value] = average_KL_per_generation\n",
    "\n",
    "# save the results\n",
    "pickle.dump(\n",
    "    final_map,\n",
    "    open(\"/Users/marinamancoridis/Thesis/Thesis_Simulations/final_map_new.p\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e3ff0",
   "metadata": {},
   "source": [
    "## Which parameters give us the publication crisis?\n",
    "\n",
    "This analysis looks at the percentage of false publications and finds the settings that match the rate reported in science in 2015: around â…“.\n",
    "- Limit to one generation\n",
    "- Define the false publication rate to be the average absolute deviation between the true and published rates across bins\n",
    "- One interesting result would be to see that you need super high surprise values to get you there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb825b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "num_bins = 3\n",
    "num_draws = 10\n",
    "num_participants = 10\n",
    "num_generations = 3\n",
    "\n",
    "# distribution of bins\n",
    "bins_to_probs = {}\n",
    "for i in range(0, num_bins):\n",
    "    bins_to_probs[i] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e68344e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aim is to graph scalar for surprise against false publication rate (also graph y = 0.3)\n",
    "# plot a line corresponding to different scalar values for the amount of data...\n",
    "# ... this will tell you how well an increase in data is able to curtail false publication values\n",
    "surprise_values = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "data_values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b677fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false publication rate = % of reports published with \n",
    "\n",
    "def run_experiment_publication_crisis(setting, alpha_value, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val):\n",
    "    scientific_record = {}\n",
    "    total_false_published = 0\n",
    "    total_true_published = 0\n",
    "    \n",
    "    for bin_num in range(0, num_bins):\n",
    "        scientific_record[bin_num] = {} \n",
    "        scientific_record[bin_num][0] = 1\n",
    "        scientific_record[bin_num][1] = 1\n",
    "        \n",
    "    for generation in range(0, num_generations):      \n",
    "        helper.print_record(scientific_record, num_bins)\n",
    "        # each generation gets an entirely new set of participants\n",
    "        participants = make_participants(setting, alpha_value)\n",
    "\n",
    "        # scientists explore and submit reports\n",
    "        for participant in participants:\n",
    "            # sample\n",
    "            for i in range(0, num_draws):\n",
    "                bin_number, value = participant.sample(scientific_record, num_bins, bins_to_probs)\n",
    "\n",
    "            # choose the bin\n",
    "            bin_choice = participant.choose_bin(scientific_record, num_bins, num_draws)\n",
    "\n",
    "            # make a report\n",
    "            participant.report(num_bins, num_draws)\n",
    "            \n",
    "        # the peer review board selects reports for publication and returns the updated scientific record\n",
    "        scientific_record, num_true_published, num_false_published = publisher.peer_review_with_fpr(participants, scientific_record, rel_pl_data_val, rel_pl_surprise_val, rel_pl_bias_val, num_draws, bins_to_probs)\n",
    "        total_false_published += num_false_published\n",
    "        total_true_published += num_true_published\n",
    "       \n",
    "    return(total_false_published / (total_false_published + total_true_published)) # return average false publication rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be5c4ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientific record\n",
      "   bin 0: 1 zero(s), 1 one(s)\n",
      "   bin 1: 1 zero(s), 1 one(s)\n",
      "   bin 2: 1 zero(s), 1 one(s)\n",
      "\n",
      "REPORTED BIN: 0 ZEROS, 1 ONES\n",
      "p value: 0.5\n",
      "it was a true publication\n",
      "REPORTED BIN: 1 ZEROS, 3 ONES\n",
      "p value: 0.11111111111111105\n",
      "it was a true publication\n",
      "Scientific record\n",
      "   bin 0: 1 zero(s), 1 one(s)\n",
      "   bin 1: 1 zero(s), 1 one(s)\n",
      "   bin 2: 2 zero(s), 5 one(s)\n",
      "\n",
      "REPORTED BIN: 1 ZEROS, 7 ONES\n",
      "p value: 0.03515625\n",
      "it was a false publication\n",
      "REPORTED BIN: 2 ZEROS, 1 ONES\n",
      "p value: 0.4879999999999999\n",
      "it was a true publication\n",
      "Scientific record\n",
      "   bin 0: 4 zero(s), 9 one(s)\n",
      "   bin 1: 1 zero(s), 1 one(s)\n",
      "   bin 2: 2 zero(s), 5 one(s)\n",
      "\n",
      "REPORTED BIN: 4 ZEROS, 0 ONES\n",
      "p value: 1.0\n",
      "it was a true publication\n",
      "REPORTED BIN: 1 ZEROS, 0 ONES\n",
      "p value: 1.0\n",
      "it was a true publication\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment_publication_crisis(\"data\", 0, 1, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ec43b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "\n",
    "for surprise_val in surprise_values:\n",
    "    print(surprise_val)\n",
    "    for data_val in data_values:\n",
    "        # take the average across ten experiments\n",
    "        for i in range(0, 10):\n",
    "            false_pub_rate += run_experiment_publication_crisis(\"data\", 0, data_val, surprise_val, 0)\n",
    "        key = (surprise_val, data_val)\n",
    "        fpr[key] = false_pub_rate / 10\n",
    "        \n",
    "# save the results\n",
    "pickle.dump(\n",
    "    fpr,\n",
    "    open(\"/Users/marinamancoridis/Thesis/Thesis_Simulations/fpr.p\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21507842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
